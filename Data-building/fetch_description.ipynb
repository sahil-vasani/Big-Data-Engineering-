{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1784e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your CSVimport pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"Acc_Date\",\n",
    "    \"Acc_No\",\n",
    "    \"Title\",\n",
    "    \"ISBN\",\n",
    "    \"Author_Editor\",\n",
    "    \"Edition_Volume\",\n",
    "    \"Place_Publisher\",\n",
    "    \"Year\",\n",
    "    \"Pages\",\n",
    "    \"Class_No\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"dau_library_data.csv\",\n",
    "    usecols=range(len(columns)),  \n",
    "    names=columns,\n",
    "    header=0,\n",
    "    encoding='latin1'\n",
    ")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(subset=[\"Title\",\n",
    "    \"ISBN\",\n",
    "    \"Author_Editor\",\n",
    "    \"Edition_Volume\",\n",
    "    \"Place_Publisher\",\n",
    "    \"Year\",\n",
    "    \"Pages\",\n",
    "    \"Class_No\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a94a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Library Data fetch through ISBN 5000 rows \n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "df10 = df.iloc[:5000].copy()\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; Sahil-OpenLibrary-PTag/Debug)\"\n",
    "}\n",
    "\n",
    "for i, row in df10.iterrows():\n",
    "    isbn = str(row.get(\"ISBN\", \"\")).strip()\n",
    "    print(f\"\\nRow {i+1} | ISBN: {isbn}\")\n",
    "\n",
    "    desc = \"ISBN Not Matched\"\n",
    "\n",
    "    if isbn:\n",
    "        url = f\"https://openlibrary.org/isbn/{isbn}\"\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=10, allow_redirects=True)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                print(\"ISBN page found ✅\")\n",
    "\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                p_tag = soup.select_one(\n",
    "                    \"div.book-description div.read-more__content p\"\n",
    "                )\n",
    "\n",
    "                if p_tag:\n",
    "                    desc = p_tag.get_text(strip=True)\n",
    "                    print(\"Description extracted ✅\")\n",
    "                else:\n",
    "                    desc = \"Description Not Available\"\n",
    "                    print(\"Description missing ❌\")\n",
    "\n",
    "            else:\n",
    "                \n",
    "                print(\"ISBN not found ❌\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    else:\n",
    "        print(\"ISBN missing in CSV ❌\")\n",
    "\n",
    "    descriptions.append(desc)\n",
    "    time.sleep(1)\n",
    "\n",
    "df10[\"description\"] = descriptions\n",
    "df10.to_csv(\"OpenLibrary_5000_rows.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved CSV with correct description labels ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a090d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML tag Through Google books throguh ISBN \n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    " \n",
    "df100 = df\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "def clean_isbn(isbn):\n",
    "    return re.sub(r\"[^0-9Xx]\", \"\", str(isbn))\n",
    "\n",
    "for i, isbn in enumerate(df100[\"ISBN\"]):\n",
    "    isbn = clean_isbn(isbn)\n",
    "    print(f\"Processing Row {i+1} | ISBN: {isbn}\")\n",
    "\n",
    "    desc = \"Not Found\"\n",
    "\n",
    "    if isbn:\n",
    "        url = f\"https://books.google.com/books?vid=ISBN{isbn}\"\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            desc_div = soup.find(\"div\", id=\"synopsis\")\n",
    "            if desc_div:\n",
    "                desc = desc_div.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    descriptions.append(desc)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "df100[\"description\"] = descriptions\n",
    "df100.to_csv(\"HTML_tag_through_All_36000.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved first 100 rows with descriptions ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c53c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('OpenLibrary_5000_rows.csv') \n",
    "isbn_not_matched = (df[\"description\"] == \"ISBN Not Matched\").sum()\n",
    "desc_not_available = (df[\"description\"] == \"Description Not Available\").sum()\n",
    "\n",
    "print(\"Desc Not availble out of 5000 row\",desc_not_available)  \n",
    "print(\"ISBN not found out of 5000 row\",isbn_not_matched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('HTML_tag_through_All_36000.csv') \n",
    "isbn_not_matched = (df[\"description\"] == \"Not Found\").sum() \n",
    "print(\"not found In HTML tag through \",isbn_not_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066366a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"HTML_tag_through_All_36000.csv\")\n",
    "df2 = pd.read_csv(\"OpenLibrary_5000_rows.csv\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"desc1\": df1[\"description\"],\n",
    "    \"desc2\": df2[\"description\"]\n",
    "})\n",
    "\n",
    "missing_vals = [\"Not Found\", \"ISBN Not Matched\", \"Description Not Available\"]\n",
    " \n",
    "both_missing = (df[\"desc1\"].isin(missing_vals) & df[\"desc2\"].isin(missing_vals)).sum()\n",
    " \n",
    "left_missing_right_has = (df[\"desc1\"].isin(missing_vals) & ~df[\"desc2\"].isin(missing_vals)).sum()\n",
    " \n",
    "right_missing_left_has = (df[\"desc2\"].isin(missing_vals) & ~df[\"desc1\"].isin(missing_vals)).sum()\n",
    " \n",
    "both_same = (df[\"desc1\"] == df[\"desc2\"]).sum()\n",
    " \n",
    "both_same_missing = (\n",
    "    (df[\"desc1\"] == df[\"desc2\"]) & df[\"desc1\"].isin(missing_vals)\n",
    ").sum()\n",
    "\n",
    "print(\"Both Missing:\", both_missing)\n",
    "print(\"Left Missing but Right Has:\", left_missing_right_has)\n",
    "print(\"Right Missing but Left Has:\", right_missing_left_has)\n",
    "print(\"Both Descriptions Same:\", both_same)\n",
    "print(\"Both Same & Missing:\", both_same_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dcc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "html_df = pd.read_csv(\"HTML_tag_through_All_36000.csv\")\n",
    "ol_df = pd.read_csv(\"OpenLibrary_5000_rows.csv\")\n",
    "\n",
    "missing_vals = [\"Not Found\", \"ISBN Not Matched\", \"Description Not Available\"]\n",
    " \n",
    "ol_desc_aligned = ol_df[\"description\"].reindex(html_df.index)\n",
    " \n",
    "mask = html_df[\"description\"].isin(missing_vals) & ~ol_desc_aligned.isin(missing_vals)\n",
    "\n",
    "html_df.loc[mask, \"description\"] = ol_desc_aligned[mask]\n",
    "\n",
    "html_df.to_csv(\"Final_Merged_Descriptions.csv\", index=False)\n",
    "\n",
    "print(\"Merged successfully Sahil ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"Final_Merged_Descriptions.csv\")\n",
    "df2 = pd.read_csv(\"OpenLibrary_5000_rows.csv\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"desc1\": df1[\"description\"],\n",
    "    \"desc2\": df2[\"description\"]\n",
    "})\n",
    "\n",
    "missing_vals = [\"Not Found\", \"ISBN Not Matched\", \"Description Not Available\"]\n",
    " \n",
    "both_missing = (df[\"desc1\"].isin(missing_vals) & df[\"desc2\"].isin(missing_vals)).sum()\n",
    " \n",
    "left_missing_right_has = (df[\"desc1\"].isin(missing_vals) & ~df[\"desc2\"].isin(missing_vals)).sum()\n",
    " \n",
    "right_missing_left_has = (df[\"desc2\"].isin(missing_vals) & ~df[\"desc1\"].isin(missing_vals)).sum()\n",
    " \n",
    "both_same = (df[\"desc1\"] == df[\"desc2\"]).sum()\n",
    " \n",
    "both_same_missing = (\n",
    "    (df[\"desc1\"] == df[\"desc2\"]) & df[\"desc1\"].isin(missing_vals)\n",
    ").sum()\n",
    "\n",
    "print(\"Both Missing:\", both_missing)\n",
    "print(\"Left Missing but Right Has:\", left_missing_right_has)\n",
    "print(\"Right Missing but Left Has:\", right_missing_left_has)\n",
    "print(\"Both Descriptions Same:\", both_same)\n",
    "print(\"Both Same & Missing:\", both_same_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3fd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title + Author through Gooogle Bookss\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    " \n",
    "df = pd.read_csv(\"Final_Merged_descriptions.csv\")\n",
    "\n",
    "original_nf = df[\n",
    "    (df[\"description\"].isna()) |\n",
    "    (df[\"description\"].astype(str).str.strip() == \"\") |\n",
    "    (df[\"description\"] == \"Not Found\")\n",
    "].copy()\n",
    "\n",
    "original_nf=original_nf[:50].copy() \n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "df[\"clean_title\"] = df[\"Title\"].apply(clean_text)\n",
    "df[\"clean_author\"] = df[\"Author_Editor\"].apply(clean_text)\n",
    "\n",
    " \n",
    "if \"Publisher\" in df.columns:\n",
    "    df[\"clean_publisher\"] = df[\"Publisher\"].apply(clean_text)\n",
    "else:\n",
    "    df[\"clean_publisher\"] = \"\"\n",
    "\n",
    " \n",
    "def google_books_api_search(query):\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q={query}&maxResults=1\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10).json()\n",
    "        items = res.get(\"items\")\n",
    "        if not items:\n",
    "            return None\n",
    "        return items[0].get(\"volumeInfo\", {}).get(\"description\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    " \n",
    "def get_google_books_description(title, author, publisher):\n",
    "    queries = []\n",
    "\n",
    "    \n",
    "    if author:\n",
    "        queries.append(f\"intitle:{title}+inauthor:{author}\")\n",
    "\n",
    "   \n",
    "    if author:\n",
    "        queries.append(f\"intitle:{title}+{author}\")\n",
    "\n",
    "    \n",
    "    if publisher:\n",
    "        queries.append(f\"intitle:{title}+inpublisher:{publisher}\")\n",
    "\n",
    "     \n",
    "    queries.append(f\"intitle:{title}\")\n",
    "\n",
    "    for q in queries:\n",
    "        desc = google_books_api_search(quote_plus(q))\n",
    "        if desc and len(desc) > 50:\n",
    "            return desc\n",
    "\n",
    "    return None\n",
    "\n",
    " \n",
    "for idx, row in original_nf.iterrows():\n",
    "\n",
    "    print(\"Processing:\", row[\"Title\"])\n",
    "\n",
    "    desc = get_google_books_description(\n",
    "        df.at[idx, \"clean_title\"],\n",
    "        df.at[idx, \"clean_author\"],\n",
    "        df.at[idx, \"clean_publisher\"]\n",
    "    )\n",
    "\n",
    "    if desc:\n",
    "        df.at[idx, \"description\"] = desc\n",
    "    else:\n",
    "        df.at[idx, \"description\"] = \"Not Found\"\n",
    "\n",
    "    time.sleep(0.2)\n",
    "\n",
    " \n",
    "final_output = original_nf[[\"Title\", \"Author_Editor\"]].copy()\n",
    "final_output[\"description\"] = df.loc[original_nf.index, \"description\"]\n",
    "\n",
    "final_output.to_csv(\"Final_GoogleBooks_Descriptions.csv\", index=False)\n",
    "\n",
    "print(\"All done Sahil ✅ Google Books description extracted successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match with Final_GoogleBooks_Descriptions.csv and copy into Final_Merged_Descriptions.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    " \n",
    "source = pd.read_csv(\"Final_GoogleBooks_Descriptions.csv\")   # has correct descriptions\n",
    "target = pd.read_csv(\"Final_Merged_Descriptions.csv\")   # has Not Found / null descriptions\n",
    "\n",
    " \n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).strip().lower()\n",
    "\n",
    "for col in [\"Title\", \"Author_Editor\"]:\n",
    "    source[col] = source[col].apply(clean_text)\n",
    "    target[col] = target[col].apply(clean_text)\n",
    "\n",
    " \n",
    "source = source.rename(columns={\"description\": \"description_src\"})\n",
    "\n",
    " \n",
    "merged = target.merge(\n",
    "    source[[\"Title\", \"Author_Editor\", \"description_src\"]],\n",
    "    on=[\"Title\", \"Author_Editor\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    " \n",
    "mask = (\n",
    "    merged[\"description\"].isna() |\n",
    "    (merged[\"description\"].astype(str).str.strip() == \"\") |\n",
    "    (merged[\"description\"] == \"Not Found\")\n",
    ")\n",
    "\n",
    "merged.loc[mask, \"description\"] = merged.loc[mask, \"description_src\"]\n",
    "\n",
    " \n",
    "merged = merged.drop(columns=[\"description_src\"])\n",
    "\n",
    " \n",
    "merged.to_csv(\"dau_with_description.csv\", index=False)\n",
    "\n",
    "print(\"Done Sahil ✅ Description updated only where match exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c538805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32377, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dau_with_description.csv\")\n",
    "df=df.drop_duplicates(subset=[\"Title\",\n",
    "    \"ISBN\",\n",
    "    \"Author_Editor\",\n",
    "    \"Edition_Volume\",\n",
    "    \"Place_Publisher\",\n",
    "    \"Year\",\n",
    "    \"Pages\",\n",
    "    \"Class_No\"])\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62619963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3189, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_nf = df[\n",
    "    (df[\"description\"].isna()) |\n",
    "    (df[\"description\"].astype(str).str.strip() == \"\") |\n",
    "    (df[\"description\"] == \"Not Found\")\n",
    "].copy()\n",
    "original_nf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc43b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dau_with_description.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdau_with_description.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[1;32mc:\\Users\\Sahil Vasani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sahil Vasani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Sahil Vasani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sahil Vasani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Sahil Vasani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dau_with_description.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"D:\\COLLAGE\\DAIICT\\2 - SEM\\BDE\\Project\\Big-Data-Engineering-\\Data\\processed\\dau_with_description.csv\")\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50761d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
