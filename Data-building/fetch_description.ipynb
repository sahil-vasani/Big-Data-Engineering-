{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1784e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load your CSVimport pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"Acc_Date\",\n",
    "    \"Acc_No\",\n",
    "    \"Title\",\n",
    "    \"ISBN\",\n",
    "    \"Author_Editor\",\n",
    "    \"Edition_Volume\",\n",
    "    \"Place_Publisher\",\n",
    "    \"Year\",\n",
    "    \"Pages\",\n",
    "    \"Class_No\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"dau_library_data.csv\",\n",
    "    usecols=range(len(columns)),  \n",
    "    names=columns,\n",
    "    header=0,\n",
    "    encoding='latin1'\n",
    ")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(subset=[\"Title\",\n",
    "    \"ISBN\",\n",
    "    \"Author_Editor\",\n",
    "    \"Edition_Volume\",\n",
    "    \"Place_Publisher\",\n",
    "    \"Year\",\n",
    "    \"Pages\",\n",
    "    \"Class_No\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a94a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Library Data fetch through ISBN 5000 rows \n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "df10 = df.iloc[:5000].copy()\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; Sahil-OpenLibrary-PTag/Debug)\"\n",
    "}\n",
    "\n",
    "for i, row in df10.iterrows():\n",
    "    isbn = str(row.get(\"ISBN\", \"\")).strip()\n",
    "    print(f\"\\nRow {i+1} | ISBN: {isbn}\")\n",
    "\n",
    "    desc = \"ISBN Not Matched\"\n",
    "\n",
    "    if isbn:\n",
    "        url = f\"https://openlibrary.org/isbn/{isbn}\"\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=10, allow_redirects=True)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                print(\"ISBN page found ✅\")\n",
    "\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                p_tag = soup.select_one(\n",
    "                    \"div.book-description div.read-more__content p\"\n",
    "                )\n",
    "\n",
    "                if p_tag:\n",
    "                    desc = p_tag.get_text(strip=True)\n",
    "                    print(\"Description extracted ✅\")\n",
    "                else:\n",
    "                    desc = \"Description Not Available\"\n",
    "                    print(\"Description missing ❌\")\n",
    "\n",
    "            else:\n",
    "                \n",
    "                print(\"ISBN not found ❌\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    else:\n",
    "        print(\"ISBN missing in CSV ❌\")\n",
    "\n",
    "    descriptions.append(desc)\n",
    "    time.sleep(1)\n",
    "\n",
    "df10[\"description\"] = descriptions\n",
    "df10.to_csv(\"OpenLibrary_5000_rows.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved CSV with correct description labels ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a090d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML tag Through Google books throguh ISBN \n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    " \n",
    "df100 = df\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "def clean_isbn(isbn):\n",
    "    return re.sub(r\"[^0-9Xx]\", \"\", str(isbn))\n",
    "\n",
    "for i, isbn in enumerate(df100[\"ISBN\"]):\n",
    "    isbn = clean_isbn(isbn)\n",
    "    print(f\"Processing Row {i+1} | ISBN: {isbn}\")\n",
    "\n",
    "    desc = \"Not Found\"\n",
    "\n",
    "    if isbn:\n",
    "        url = f\"https://books.google.com/books?vid=ISBN{isbn}\"\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            desc_div = soup.find(\"div\", id=\"synopsis\")\n",
    "            if desc_div:\n",
    "                desc = desc_div.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    descriptions.append(desc)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "df100[\"description\"] = descriptions\n",
    "df100.to_csv(\"HTML_tag_through_All_36000.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved first 100 rows with descriptions ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c53c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('OpenLibrary_5000_rows.csv') \n",
    "isbn_not_matched = (df[\"description\"] == \"ISBN Not Matched\").sum()\n",
    "desc_not_available = (df[\"description\"] == \"Description Not Available\").sum()\n",
    "\n",
    "print(\"Desc Not availble out of 5000 row\",desc_not_available)  \n",
    "print(\"ISBN not found out of 5000 row\",isbn_not_matched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('HTML_tag_through_All_36000.csv') \n",
    "isbn_not_matched = (df[\"description\"] == \"Not Found\").sum() \n",
    "print(\"not found In HTML tag through \",isbn_not_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066366a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"HTML_tag_through_All_36000.csv\")\n",
    "df2 = pd.read_csv(\"OpenLibrary_5000_rows.csv\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"desc1\": df1[\"description\"],\n",
    "    \"desc2\": df2[\"description\"]\n",
    "})\n",
    "\n",
    "missing_vals = [\"Not Found\", \"ISBN Not Matched\", \"Description Not Available\"]\n",
    " \n",
    "both_missing = (df[\"desc1\"].isin(missing_vals) & df[\"desc2\"].isin(missing_vals)).sum()\n",
    " \n",
    "left_missing_right_has = (df[\"desc1\"].isin(missing_vals) & ~df[\"desc2\"].isin(missing_vals)).sum()\n",
    " \n",
    "right_missing_left_has = (df[\"desc2\"].isin(missing_vals) & ~df[\"desc1\"].isin(missing_vals)).sum()\n",
    " \n",
    "both_same = (df[\"desc1\"] == df[\"desc2\"]).sum()\n",
    " \n",
    "both_same_missing = (\n",
    "    (df[\"desc1\"] == df[\"desc2\"]) & df[\"desc1\"].isin(missing_vals)\n",
    ").sum()\n",
    "\n",
    "print(\"Both Missing:\", both_missing)\n",
    "print(\"Left Missing but Right Has:\", left_missing_right_has)\n",
    "print(\"Right Missing but Left Has:\", right_missing_left_has)\n",
    "print(\"Both Descriptions Same:\", both_same)\n",
    "print(\"Both Same & Missing:\", both_same_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dcc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "html_df = pd.read_csv(\"HTML_tag_through_All_36000.csv\")\n",
    "ol_df = pd.read_csv(\"OpenLibrary_5000_rows.csv\")\n",
    "\n",
    "missing_vals = [\"Not Found\", \"ISBN Not Matched\", \"Description Not Available\"]\n",
    " \n",
    "ol_desc_aligned = ol_df[\"description\"].reindex(html_df.index)\n",
    " \n",
    "mask = html_df[\"description\"].isin(missing_vals) & ~ol_desc_aligned.isin(missing_vals)\n",
    "\n",
    "html_df.loc[mask, \"description\"] = ol_desc_aligned[mask]\n",
    "\n",
    "html_df.to_csv(\"Final_Merged_Descriptions.csv\", index=False)\n",
    "\n",
    "print(\"Merged successfully Sahil ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"Final_Merged_Descriptions.csv\")\n",
    "df2 = pd.read_csv(\"OpenLibrary_5000_rows.csv\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"desc1\": df1[\"description\"],\n",
    "    \"desc2\": df2[\"description\"]\n",
    "})\n",
    "\n",
    "missing_vals = [\"Not Found\", \"ISBN Not Matched\", \"Description Not Available\"]\n",
    " \n",
    "both_missing = (df[\"desc1\"].isin(missing_vals) & df[\"desc2\"].isin(missing_vals)).sum()\n",
    " \n",
    "left_missing_right_has = (df[\"desc1\"].isin(missing_vals) & ~df[\"desc2\"].isin(missing_vals)).sum()\n",
    " \n",
    "right_missing_left_has = (df[\"desc2\"].isin(missing_vals) & ~df[\"desc1\"].isin(missing_vals)).sum()\n",
    " \n",
    "both_same = (df[\"desc1\"] == df[\"desc2\"]).sum()\n",
    " \n",
    "both_same_missing = (\n",
    "    (df[\"desc1\"] == df[\"desc2\"]) & df[\"desc1\"].isin(missing_vals)\n",
    ").sum()\n",
    "\n",
    "print(\"Both Missing:\", both_missing)\n",
    "print(\"Left Missing but Right Has:\", left_missing_right_has)\n",
    "print(\"Right Missing but Left Has:\", right_missing_left_has)\n",
    "print(\"Both Descriptions Same:\", both_same)\n",
    "print(\"Both Same & Missing:\", both_same_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3fd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title + Author through Gooogle Bookss\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    " \n",
    "df = pd.read_csv(\"Final_Merged_descriptions.csv\")\n",
    "\n",
    "original_nf = df[\n",
    "    (df[\"description\"].isna()) |\n",
    "    (df[\"description\"].astype(str).str.strip() == \"\") |\n",
    "    (df[\"description\"] == \"Not Found\")\n",
    "].copy()\n",
    "\n",
    "original_nf=original_nf[:50].copy() \n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "df[\"clean_title\"] = df[\"Title\"].apply(clean_text)\n",
    "df[\"clean_author\"] = df[\"Author_Editor\"].apply(clean_text)\n",
    "\n",
    " \n",
    "if \"Publisher\" in df.columns:\n",
    "    df[\"clean_publisher\"] = df[\"Publisher\"].apply(clean_text)\n",
    "else:\n",
    "    df[\"clean_publisher\"] = \"\"\n",
    "\n",
    " \n",
    "def google_books_api_search(query):\n",
    "    url = f\"https://www.googleapis.com/books/v1/volumes?q={query}&maxResults=1\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10).json()\n",
    "        items = res.get(\"items\")\n",
    "        if not items:\n",
    "            return None\n",
    "        return items[0].get(\"volumeInfo\", {}).get(\"description\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    " \n",
    "def get_google_books_description(title, author, publisher):\n",
    "    queries = []\n",
    "\n",
    "    \n",
    "    if author:\n",
    "        queries.append(f\"intitle:{title}+inauthor:{author}\")\n",
    "\n",
    "   \n",
    "    if author:\n",
    "        queries.append(f\"intitle:{title}+{author}\")\n",
    "\n",
    "    \n",
    "    if publisher:\n",
    "        queries.append(f\"intitle:{title}+inpublisher:{publisher}\")\n",
    "\n",
    "     \n",
    "    queries.append(f\"intitle:{title}\")\n",
    "\n",
    "    for q in queries:\n",
    "        desc = google_books_api_search(quote_plus(q))\n",
    "        if desc and len(desc) > 50:\n",
    "            return desc\n",
    "\n",
    "    return None\n",
    "\n",
    " \n",
    "for idx, row in original_nf.iterrows():\n",
    "\n",
    "    print(\"Processing:\", row[\"Title\"])\n",
    "\n",
    "    desc = get_google_books_description(\n",
    "        df.at[idx, \"clean_title\"],\n",
    "        df.at[idx, \"clean_author\"],\n",
    "        df.at[idx, \"clean_publisher\"]\n",
    "    )\n",
    "\n",
    "    if desc:\n",
    "        df.at[idx, \"description\"] = desc\n",
    "    else:\n",
    "        df.at[idx, \"description\"] = \"Not Found\"\n",
    "\n",
    "    time.sleep(0.2)\n",
    "\n",
    " \n",
    "final_output = original_nf[[\"Title\", \"Author_Editor\"]].copy()\n",
    "final_output[\"description\"] = df.loc[original_nf.index, \"description\"]\n",
    "\n",
    "final_output.to_csv(\"Final_GoogleBooks_Descriptions.csv\", index=False)\n",
    "\n",
    "print(\"All done Sahil ✅ Google Books description extracted successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# match with Final_GoogleBooks_Descriptions.csv and copy into Final_Merged_Descriptions.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    " \n",
    "source = pd.read_csv(\"Final_GoogleBooks_Descriptions.csv\")   # has correct descriptions\n",
    "target = pd.read_csv(\"Final_Merged_Descriptions.csv\")   # has Not Found / null descriptions\n",
    "\n",
    " \n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).strip().lower()\n",
    "\n",
    "for col in [\"Title\", \"Author_Editor\"]:\n",
    "    source[col] = source[col].apply(clean_text)\n",
    "    target[col] = target[col].apply(clean_text)\n",
    "\n",
    " \n",
    "source = source.rename(columns={\"description\": \"description_src\"})\n",
    "\n",
    " \n",
    "merged = target.merge(\n",
    "    source[[\"Title\", \"Author_Editor\", \"description_src\"]],\n",
    "    on=[\"Title\", \"Author_Editor\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    " \n",
    "mask = (\n",
    "    merged[\"description\"].isna() |\n",
    "    (merged[\"description\"].astype(str).str.strip() == \"\") |\n",
    "    (merged[\"description\"] == \"Not Found\")\n",
    ")\n",
    "\n",
    "merged.loc[mask, \"description\"] = merged.loc[mask, \"description_src\"]\n",
    "\n",
    " \n",
    "merged = merged.drop(columns=[\"description_src\"])\n",
    "\n",
    " \n",
    "merged.to_csv(\"dau_with_description.csv\", index=False)\n",
    "\n",
    "print(\"Done Sahil ✅ Description updated only where match exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c538805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32377, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dau_with_description.csv\")\n",
    "df=df.drop_duplicates(subset=[\"Title\",\n",
    "    \"ISBN\",\n",
    "    \"Author_Editor\",\n",
    "    \"Edition_Volume\",\n",
    "    \"Place_Publisher\",\n",
    "    \"Year\",\n",
    "    \"Pages\",\n",
    "    \"Class_No\"])\n",
    "df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62619963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3189, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_nf = df[\n",
    "    (df[\"description\"].isna()) |\n",
    "    (df[\"description\"].astype(str).str.strip() == \"\") |\n",
    "    (df[\"description\"] == \"Not Found\")\n",
    "].copy()\n",
    "original_nf.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
