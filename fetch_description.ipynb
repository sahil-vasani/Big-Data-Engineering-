{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa88bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch from open Library\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "df10 = df.iloc[:5000].copy()\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; Sahil-OpenLibrary-PTag/Debug)\"\n",
    "}\n",
    "\n",
    "for i, row in df10.iterrows():\n",
    "    isbn = str(row.get(\"ISBN\", \"\")).strip()\n",
    "    print(f\"\\nRow {i+1} | ISBN: {isbn}\")\n",
    "\n",
    "    desc = \"ISBN Not Matched\"\n",
    "\n",
    "    if isbn:\n",
    "        url = f\"https://openlibrary.org/isbn/{isbn}\"\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=10, allow_redirects=True)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                print(\"ISBN page found ✅\")\n",
    "\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                p_tag = soup.select_one(\n",
    "                    \"div.book-description div.read-more__content p\"\n",
    "                )\n",
    "\n",
    "                if p_tag:\n",
    "                    desc = p_tag.get_text(strip=True)\n",
    "                    print(\"Description extracted ✅\")\n",
    "                else:\n",
    "                    desc = \"Description Not Available\"\n",
    "                    print(\"Description missing ❌\")\n",
    "\n",
    "            else:\n",
    "                \n",
    "                print(\"ISBN not found ❌\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    else:\n",
    "        print(\"ISBN missing in CSV ❌\")\n",
    "\n",
    "    descriptions.append(desc)\n",
    "    time.sleep(1)\n",
    "\n",
    "df10[\"description\"] = descriptions\n",
    "df10.to_csv(\"OpenLibrary_5000_rows.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved CSV with correct description labels ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d151a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML tag Through using google books colab \n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    " \n",
    "df100 = df\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "def clean_isbn(isbn):\n",
    "    return re.sub(r\"[^0-9Xx]\", \"\", str(isbn))\n",
    "\n",
    "for i, isbn in enumerate(df100[\"ISBN\"]):\n",
    "    isbn = clean_isbn(isbn)\n",
    "    print(f\"Processing Row {i+1} | ISBN: {isbn}\")\n",
    "\n",
    "    desc = \"Not Found\"\n",
    "\n",
    "    if isbn:\n",
    "        url = f\"https://books.google.com/books?vid=ISBN{isbn}\"\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "            desc_div = soup.find(\"div\", id=\"synopsis\")\n",
    "            if desc_div:\n",
    "                desc = desc_div.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "    descriptions.append(desc)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "df100[\"description\"] = descriptions\n",
    "df100.to_csv(\"HTML_tag_through_All_36000.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved first 100 rows with descriptions ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge HTML TAG  side\n",
    "import pandas as pd\n",
    "\n",
    "html_df = pd.read_csv(\"HTML_tag_through_All_36000.csv\")\n",
    "ol_df = pd.read_csv(\"OpenLibrary_5000_rows.csv\")\n",
    "\n",
    "missing_vals = [\"Not Found\", \"ISBN Not Matched\", \"Description Not Available\"]\n",
    "\n",
    "# Align OL descriptions to HTML by index\n",
    "ol_desc_aligned = ol_df[\"description\"].reindex(html_df.index)\n",
    "\n",
    "# Replace where HTML missing and OL has valid description\n",
    "mask = html_df[\"description\"].isin(missing_vals) & ~ol_desc_aligned.isin(missing_vals)\n",
    "\n",
    "html_df.loc[mask, \"description\"] = ol_desc_aligned[mask]\n",
    "\n",
    "html_df.to_csv(\"Final_Merged_Descriptions.csv\", index=False)\n",
    "\n",
    "print(\"Merged successfully Sahil ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64126ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find open Library through title +Author through \n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"Final_Merged_descriptions.csv\")\n",
    "\n",
    "# Rows where description is missing OR Not Found\n",
    "original_nf = df[\n",
    "    (df[\"description\"].isna()) |\n",
    "    (df[\"description\"].astype(str).str.strip() == \"\") |\n",
    "    (df[\"description\"] == \"Not Found\")\n",
    "].copy()\n",
    "\n",
    "original_nf=original_nf[:500].copy()\n",
    " \n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# Clean title and author\n",
    "df[\"clean_title\"] = df[\"Title\"].apply(clean_text)\n",
    "df[\"clean_author\"] = df[\"Author_Editor\"].apply(clean_text)\n",
    "\n",
    "# Search OpenLibrary\n",
    "def search_openlibrary(title, author):\n",
    "    query = quote(f\"{title} {author}\")\n",
    "    url = f\"https://openlibrary.org/search?q={query}\"\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        link = soup.select_one(\"a.results\")\n",
    "        if link:\n",
    "            return \"https://openlibrary.org\" + link[\"href\"]\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    " \n",
    "def get_description(book_url):\n",
    "    try:\n",
    "        res = requests.get(book_url, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        desc = soup.select_one(\"div.read-more__content.markdown-content\")\n",
    "        if desc:\n",
    "            return desc.get_text(strip=True)\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Process only missing description rows\n",
    "for idx, row in original_nf.iterrows():\n",
    "\n",
    "    current_desc = df.at[idx, \"description\"]\n",
    "\n",
    "    # ✅ Skip if description already exists\n",
    "    if pd.notna(current_desc) and str(current_desc).strip() not in [\"\", \"Not Found\"]:\n",
    "        print(\"Skipping (already available):\", row[\"Title\"])\n",
    "        continue\n",
    "\n",
    "    print(\"Processing:\", row[\"Title\"])\n",
    "\n",
    "    book_url = search_openlibrary(\n",
    "        df.at[idx, \"clean_title\"],\n",
    "        df.at[idx, \"clean_author\"]\n",
    "    )\n",
    "\n",
    "    if book_url:\n",
    "        desc = get_description(book_url)\n",
    "\n",
    "        if desc:\n",
    "            df.at[idx, \"description\"] = desc\n",
    "        else:\n",
    "            df.at[idx, \"description\"] = \"Description not available\"\n",
    "    else:\n",
    "        df.at[idx, \"description\"] = \"Not Found\"\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Final output only for originally missing rows\n",
    "final_output = original_nf[[\"Title\", \"Author_Editor\"]].copy()\n",
    "final_output[\"description\"] = df.loc[original_nf.index, \"description\"]\n",
    "\n",
    "# Save result\n",
    "final_output.to_csv(\"Final_NotFound_With_Scraped.csv\", index=False)\n",
    "\n",
    "print(\"All done Sahil ✅ File ready\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
